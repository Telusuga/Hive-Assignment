Will the reducer work or not if you use “Limit 1” in any HiveQL query?

Answer:No the reduces will not work as we are not aggregating or grouping any of the results from mapper.



Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time? 

Answer: Apache Hive runs on Derby Db which acts as Metastore for Hive. The only disadvantage for Derby DB it doesnot have feature to handle multiple clients at once only a single client per session is allowed. If multiple clients try to work on Derby DB they will be in wait state.


Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?

Answer:As the data is so huge so inorder to search linearly we can partition the data based on column which is frequently used and on top of partitioning we can perform clustering by defining a column in which a specfic function segregates the data for faster querying and finally we can create a table with the format ORC or parquet for better compression and querying.


How can you add a new partition for the month December in the above partitioned table?

Answer:
CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’  PARTITIONED BY (month string);

insert overwrite table transaction_details partition(month='December') select cust_id,amount,month,country from transaction_details where month='December';


I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?

Answer:
This error is due to the property hive.mapred.mode=true which is for static partition has the above clearly states about using dynamic partition so in order to avoid the problem we need to set the property hive.exec.dynamic.partition.mode=nonstrict; then the error will not come.

Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?

Answer:
First while crreating the table we should use the library for the CSVSerDe
Library-->org.apache.hadoop.hive.serde2.OpenCSVSerde
and we need to see a sample data and we need to create custom properties for the data like separatorcharacters,quotecharacters,escapecharacters like
with SerdeProperties(
'SeparatorChar'=','
)


Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?

Answer:
First we need to move all the corresponding files to the into a single directory and we need to mention only the directory while loading data into table.
load data local inpath 'file:///directory/' into table <destination_tablle>
Since the table is created on top of multiple files so the data is in multiple files.
The number of mappers will also increase as there are multiple files and based on mappers there will be reducers based on the data and query.


LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;

The following statement failed to execute. What can be the cause?

Answer:There might be 2 reasons for the failed execution.
1st:The file data file may not be present at the given location.
2nd:The schema might not be matched with the input data.
3rd:The delimiters mentioned might be incorrect.

Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?

Answer: Yes we can add the data nodes into an existing cluster by using Cloudera Manager.
First we need to get the host IP Addresses of the data nodes and 
we need to add the IP Addresses of these nodes into the cluster and after adding we need to assign the role of Data Node in the cluster.

Hive Join operations

Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
)
Answer:
hive> select * from customers;
OK
customers.id    customers.name  customers.age   customers.address       customers.salary
1       Surya   23      Guntur  3.36
2       Deepak  23      Bangalore       3.39
3       Nani    24      Chennai 4.12
4       Aravind 24      Hyderabad       5.64
5       Kenny   25      Mumbai  6.52
6       Hemanth 26      Delhi   7.12
NULL    NULL    NULL    NULL    NULL
Time taken: 0.49 seconds, Fetched: 7 row(s)
hive> select * from order;
OK
order.id        order.date      order.customer_id       order.amount
1       2022-09-26      1783    50
2       2021-01-08      1793    100
3       2024-07-23      1823    501
6       2009-10-19      1943    654
Time taken: 0.156 seconds, Fetched: 4 row(s)
hive>


hive> select * from customers c inner join order o on c.id=o.id;
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
c.id    c.name  c.age   c.address       c.salary        o.id    o.date  o.customer_id   o.amount
1       Surya   23      Guntur  3.36    1       2022-09-26      1783    50
2       Deepak  23      Bangalore       3.39    2       2021-01-08      1793    100
3       Nani    24      Chennai 4.12    3       2024-07-23      1823    501
6       Hemanth 26      Delhi   7.12    6       2009-10-19      1943    654

hive> select * from customers c left outer join order o on c.id=o.id;
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
c.id    c.name  c.age   c.address       c.salary        o.id    o.date  o.customer_id   o.amount
NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL
1       Surya   23      Guntur  3.36    1       2022-09-26      1783    50
2       Deepak  23      Bangalore       3.39    2       2021-01-08      1793    100
3       Nani    24      Chennai 4.12    3       2024-07-23      1823    501
4       Aravind 24      Hyderabad       5.64    NULL    NULL    NULL    NULL
5       Kenny   25      Mumbai  6.52    NULL    NULL    NULL    NULL
6       Hemanth 26      Delhi   7.12    6       2009-10-19      1943    654


hive> select * from customers c right outer join order o on c.id=o.id;
c.id    c.name  c.age   c.address       c.salary        o.id    o.date  o.customer_id   o.amount
1       Surya   23      Guntur  3.36    1       2022-09-26      1783    50
2       Deepak  23      Bangalore       3.39    2       2021-01-08      1793    100
3       Nani    24      Chennai 4.12    3       2024-07-23      1823    501
6       Hemanth 26      Delhi   7.12    6       2009-10-19      1943    654


hive> select * from customers c full outer join order o on c.id=o.id;
c.id    c.name  c.age   c.address       c.salary        o.id    o.date  o.customer_id   o.amount
NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL
1       Surya   23      Guntur  3.36    1       2022-09-26      1783    50
2       Deepak  23      Bangalore       3.39    2       2021-01-08      1793    100
3       Nani    24      Chennai 4.12    3       2024-07-23      1823    501
4       Aravind 24      Hyderabad       5.64    NULL    NULL    NULL    NULL
5       Kenny   25      Mumbai  6.52    NULL    NULL    NULL    NULL
6       Hemanth 26      Delhi   7.12    6       2009-10-19      1943    654

BUILD A DATA PIPELINE WITH HIVE

The Data contains "," as a decimal point instead of "." so I had to manually change it in the csv itself.
The first column is a date column and is in wrong format(dd/mm/yyyy) so I had to create a temp table(air_quality_initial) to take the data as string.

hive> create table air_quality_inital(
    > Date string,
    > Time string,
    > CO float,
    > PT08_S1 int,
    > NHHC int,
    > C6H6 float,
    > PT08_S2 int,
    > NOx int,
    > PT08_S3 int,
    > NO2 int,
    > PT08_S4 int,
    > PT08_S5 int,
    > T float,
    > RH float,
    > AH float
    > )
    > row format delimited
    > fields terminated by '\;'
    > tblproperties("skip.header.line.count"="1");
OK
Time taken: 4.965 seconds
hive>

Load the data from local to table
hive> load data local inpath '/home/cloudera/kaggle_covid/AirQualityUCI.csv' into table air_quality_inital;
Loading data to table my_first_hive_db.air_quality_inital
Table my_first_hive_db.air_quality_inital stats: [numFiles=1, totalSize=785065]
OK
Time taken: 6.266 seconds
hive>


Now created Actual table(Air_Quality) with date column

hive> create table Air_Quality(
    > Date date,
    > Time string,
    > CO float,
    > PT08_S1 int,
    > NHHC int,
    > C6H6 float,
    > PT08_S2 int,
    > NOx int,
    > PT08_S3 int,
    > NO2 int,
    > PT08_S4 int,
    > PT08_S5 int,
    > T float,
    > RH float,
    > AH float
    > );
OK
Time taken: 0.996 seconds
hive>

Now we need to load the data from temp table to actual table.

hive> from air_quality_inital insert overwrite table Air_Quality select date(from_unixtime(unix_timestamp(Date,'dd/mm/yyyy'), 'yyyy-mm-dd')),
    > Time,
    > CO,
    > PT08_S1,
    > NHHC,
    > C6H6,
    > PT08_S2,
    > NOx,
    > PT08_S3,
    > NO2,
    > PT08_S4,
    > PT08_S5,
    > T,
    > RH,
    > AH;

3. Perform a select operation . 

hive> select * from Air_Quality limit 5;
OK
air_quality.date        air_quality.time        air_quality.co  air_quality.pt08_s1     air_quality.nhhc        air_quality.c6h6        air_quality.pt08_s2     air_quality.nox air_quality.pt08_s3     air_quality.no2 air_quality.pt08_s4 air_quality.pt08_s5      air_quality.t   air_quality.rh  air_quality.ah
2004-03-10      18.00.00        2.6     1360    150     11.9    1046    166     1056    113     1692    1268    13.6    48.9    0.7578
2004-03-10      19.00.00        2.0     1292    112     9.4     955     103     1174    92      1559    972     13.3    47.7    0.7255
2004-03-10      20.00.00        2.2     1402    88      9.0     939     131     1140    114     1555    1074    11.9    54.0    0.7502
2004-03-10      21.00.00        2.2     1376    80      9.2     948     172     1092    122     1584    1203    11.0    60.0    0.7867
2004-03-10      22.00.00        1.6     1272    51      6.5     836     131     1205    116     1490    1110    11.2    59.6    0.7888


4. Fetch the result of the select operation in your local as a csv file 

hive> insert overwrite local directory '/home/cloudera/output' select date from Air_Quality limit 5;
Copying data to local directory /home/cloudera/output
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 7.13 sec   HDFS Read: 11598 HDFS Write: 55 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 130 msec
OK
date
Time taken: 393.492 seconds
Output in Local:
-rw-r--r-- 1 cloudera cloudera 55 Oct  9 11:36 000000_0
[cloudera@quickstart output]$ cat 000000_0
2004-03-10
2004-03-10
2004-03-10
2004-03-10
2004-03-10


5. Perform group by operation . 
hive> select DATE from air_quality group by DATE limit 10;
date
NULL
2004-03-10
2004-03-11
2004-03-12
2004-03-13
2004-03-14
2004-03-15
2004-03-16
2004-03-17
2004-03-18
Time taken: 129.368 seconds, Fetched: 10 row(s)
hive>


7. Perform filter operation at least 5 kinds of filter examples . 
hive> select nox from Air_Quality where time like '%1%' and year(date)=2004 and pt08_s1>1000 and c6h6 between 9 and 10 group by nox order by nox limit 5;
OK
nox
-200
51
54
58
59
Time taken: 87.159 seconds, Fetched: 5 row(s)
hive>


9. alter table operation
hive> alter table air_quality_3 rename to air;
OK
Time taken: 1.585 seconds
hive>

10 . drop table operation
hive> drop table air_quality_2;
OK
Time taken: 15.197 seconds
hive>

12 . order by operation .
hive> select c6h6 from Air_Quality group by c6h6 order by c6h6 desc limit 5;
Query ID = cloudera_20221010222020_01d95a7d-076f-438f-a29b-e2449d620192
Total jobs = 2
Launching Job 1 out of 2
OK
c6h6
63.7
52.1
50.8
50.7
50.6
Time taken: 486.025 seconds, Fetched: 5 row(s)
hive>

13 . where clause operations you have to perform . 
hive> select * from Air_Quality where year(date)=2004 limit 5;
OK
air_quality.date        air_quality.time        air_quality.co  air_quality.pt08_s1     air_quality.nhhc        air_quality.c6h6        air_quality.pt08_s2     air_quality.nox air_quality.pt08_s3     air_quality.no2 air_quality.pt08_s4 air_quality.pt08_s5      air_quality.t   air_quality.rh  air_quality.ah
2004-03-10      18.00.00        2.6     1360    150     11.9    1046    166     1056    113     1692    1268    13.6    48.9    0.7578
2004-03-10      19.00.00        2.0     1292    112     9.4     955     103     1174    92      1559    972     13.3    47.7    0.7255
2004-03-10      20.00.00        2.2     1402    88      9.0     939     131     1140    114     1555    1074    11.9    54.0    0.7502
2004-03-10      21.00.00        2.2     1376    80      9.2     948     172     1092    122     1584    1203    11.0    60.0    0.7867
2004-03-10      22.00.00        1.6     1272    51      6.5     836     131     1205    116     1490    1110    11.2    59.6    0.7888
Time taken: 0.584 seconds, Fetched: 5 row(s)

14 . sorting operation you have to perform .
hive> select DATE from Air_Quality group by DATE sort by DATE limit 5;
OK
date
NULL
2004-03-10
2004-03-11
2004-03-12
2004-03-13
Time taken: 197.815 seconds, Fetched: 5 row(s)
hive>


15 . distinct operation you have to perform .
hive> select distinct(c6h6) from Air_Quality limit 5;
c6h6
NULL
-200.0
0.1
0.2
0.3
Time taken: 366.381 seconds, Fetched: 5 row(s)
hive>

16 . like an operation you have to perform .
hive> select * from Air_Quality where time like '%1%' limit 5;
OK
air_quality.date        air_quality.time        air_quality.co  air_quality.pt08_s1     air_quality.nhhc        air_quality.c6h6        air_quality.pt08_s2     air_quality.nox air_quality.pt08_s3     air_quality.no2 air_quality.pt08_s4 air_quality.pt08_s5      air_quality.t   air_quality.rh  air_quality.ah
2004-03-10      18.00.00        2.6     1360    150     11.9    1046    166     1056    113     1692    1268    13.6    48.9    0.7578
2004-03-10      19.00.00        2.0     1292    112     9.4     955     103     1174    92      1559    972     13.3    47.7    0.7255
2004-03-10      21.00.00        2.2     1376    80      9.2     948     172     1092    122     1584    1203    11.0    60.0    0.7867
2004-03-11      01.00.00        1.0     1136    31      3.3     672     62      1453    76      1333    730     10.7    60.0    0.7702
2004-03-11      10.00.00        1.7     1233    77      6.3     827     112     1218    98      1446    860     10.8    58.4    0.7552
Time taken: 0.113 seconds, Fetched: 5 row(s)
hive>

17 . union operation you have to perform . 
hive> select * from Air_Quality union all select * from air_quality_inital limit 5;
10/03/2004      22.00.00        1.6     1272    51      6.5     836     131     1205    116     1490    1110    11.2    59.6    0.7888
10/03/2004      21.00.00        2.2     1376    80      9.2     948     172     1092    122     1584    1203    11.0    60.0    0.7867
10/03/2004      20.00.00        2.2     1402    88      9.0     939     131     1140    114     1555    1074    11.9    54.0    0.7502
10/03/2004      19.00.00        2.0     1292    112     9.4     955     103     1174    92      1559    972     13.3    47.7    0.7255
10/03/2004      18.00.00        2.6     1360    150     11.9    1046    166     1056    113     1692    1268    13.6    48.9    0.7578
Time taken: 73.136 seconds, Fetched: 9476 row(s)
hive>


18 . table view operation you have to perform . 
hive> create or replace view Air_Quality_view as select * from Air_Quality;
OK
date    time    co      pt08_s1 nhhc    c6h6    pt08_s2 nox     pt08_s3 no2     pt08_s4 pt08_s5 t       rh      ah
Time taken: 12.45 seconds
hive> describe formatted Air_Quality_view;
OK
col_name        data_type       comment
# col_name              data_type               comment

date                    date
time                    string
co                      float
pt08_s1                 int
nhhc                    int
c6h6                    float
pt08_s2                 int
nox                     int
pt08_s3                 int
no2                     int
pt08_s4                 int
pt08_s5                 int
t                       float
rh                      float
ah                      float

# Detailed Table Information
Database:               air_quality
Owner:                  cloudera
CreateTime:             Tue Oct 11 11:50:11 PDT 2022
LastAccessTime:         UNKNOWN
Protect Mode:           None
Retention:              0
Table Type:             VIRTUAL_VIEW
Table Parameters:
        transient_lastDdlTime   1665514211

# Storage Information
SerDe Library:          null
InputFormat:            org.apache.hadoop.mapred.TextInputFormat
OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Compressed:             No
Num Buckets:            -1
Bucket Columns:         []
Sort Columns:           []

# View Information
View Original Text:     select * from Air_Quality
View Expanded Text:     select `air_quality`.`date`, `air_quality`.`time`, `air_quality`.`co`, `air_quality`.`pt08_s1`, `air_quality`.`nhhc`, `air_quality`.`c6h6`, `air_quality`.`pt08_s2`, `air_quality`.`nox`, `air_quality`.`pt08_s3`, `air_quality`.`no2`, `air_quality`.`pt08_s4`, `air_quality`.`pt08_s5`, `air_quality`.`t`, `air_quality`.`rh`, `air_quality`.`ah` from `air_quality`.`Air_Quality`
Time taken: 0.537 seconds, Fetched: 41 row(s)
hive>
